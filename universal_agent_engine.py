import os
import time
import json
import glob
from datetime import datetime
from google import genai
from dotenv import load_dotenv

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(BASE_DIR, ".env"))

# ç›£æŸ»ç”¨ãƒ­ã‚°(runs)ã‚’è¿½åŠ 
DIRS = {k: os.path.join(BASE_DIR, k) for k in ["order", "workspace", "stages", "external", "runs"]}
for d in DIRS.values(): os.makedirs(d, exist_ok=True)

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
GEMINI_MODEL = os.getenv("GEMINI_MODEL")
MAX_STEPS = 15

def get_latest_l2():
    """L2çµŒé¨“ã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—"""
    files = glob.glob(os.path.join(DIRS["workspace"], "core_experience_v*.md"))
    if not files: return "ã¾ã çµŒé¨“ã¯ãªã„ã€‚", 0
    latest_file = max(files, key=os.path.getctime)
    v_num = int(latest_file.split("_v")[-1].split(".")[0])
    return open(latest_file, "r", encoding="utf-8").read().strip(), v_num

def call_llm_json(prompt, run_dir, step_name):
    """JSONå‡ºåŠ›ã‚’å¼·åˆ¶ã—ã€å£Šã‚Œã¦ã„ãŸã‚‰ä¿®å¾©ã‚’è©¦ã¿ã‚‹å …ç‰¢ãªLLMå‘¼ã³å‡ºã—"""
    current_prompt = prompt
    for attempt in range(3):
        try:
            res = client.models.generate_content(model=GEMINI_MODEL, contents=current_prompt)
            if not res.text: raise ValueError("Empty response")
            
            raw_text = res.text.strip()
            # Markdownã® ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰¥ãŒã™
            if raw_text.startswith("```json"):
                raw_text = raw_text.split("```json")[1].split("```")[0].strip()
            elif raw_text.startswith("```"):
                raw_text = raw_text.split("```")[1].split("```")[0].strip()

            parsed_json = json.loads(raw_text)
            
            # ç›£æŸ»ãƒ­ã‚°ã®ä¿å­˜
            with open(os.path.join(run_dir, f"{step_name}_raw.txt"), "w", encoding="utf-8") as f:
                f.write(res.text)
            return parsed_json

        except json.JSONDecodeError as e:
            print(f"  âš ï¸ JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼({attempt+1}/3). è‡ªå·±ä¿®å¾©ã‚’è©¦ã¿ã¾ã™...")
            # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦ä¿®å¾©ã•ã›ã‚‹
            current_prompt = f"{prompt}\n\nã€ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã€‘å…ˆã»ã©ã®å‡ºåŠ›ã¯æœ‰åŠ¹ãªJSONã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã€å³æ ¼ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\nã‚¨ãƒ©ãƒ¼è©³ç´°: {e}"
        except Exception as e:
            if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + ã‚¸ãƒƒã‚¿ãƒ¼ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
                sleep_time = 20 * (2 ** attempt)
                print(f"  âš ï¸ APIåˆ¶é™({attempt+1}/3): {sleep_time}ç§’å¾…æ©Ÿ...")
                time.sleep(sleep_time)
            else: raise e
            
    raise RuntimeError(f"LLM failed to produce valid JSON after 3 attempts. Step: {step_name}")

def run_librarian(state, run_dir):
    """ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ä»˜ãã€‘çµŒé¨“ã®æŠ½å‡ºã¨L2ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
    print("\nğŸ§  [LIBRARIAN ACTIVE] çµŒé¨“ã®æŠ½è±¡åŒ–ã¨ L2(v{}) ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚".format(state['l2_version'] + 1))
    
    lib_prompt = f"""Role: Librarian.
ã‚ãªãŸã¯ã‚·ã‚¹ãƒ†ãƒ ã®é€²åŒ–ã‚’å¸ã‚‹è¨˜æ†¶æ•´ç†å®˜ã ã€‚
ä»¥ä¸‹ã®ã€éš”é›¢ã•ã‚ŒãŸãƒ­ã‚°ã€‘ã‹ã‚‰ã€æ™®éçš„ãªæ•™è¨“ã‚’æŠ½å‡ºã—ã€ç¾åœ¨ã®çµŒé¨“(L2)ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã›ã‚ˆã€‚
ãƒã‚¤ã‚ºã‚„å¤±æ•—ã®æ­£å½“åŒ–ã¯å¾¹åº•çš„ã«æ’é™¤ã™ã‚‹ã“ã¨ã€‚

ã€çµ¶å¯¾æ†²æ³• (Purpose)ã€‘
{state['purpose']}

--- éš”é›¢ã•ã‚ŒãŸãƒ­ã‚° (ã“ã“ã‹ã‚‰ä¸‹ã®æŒ‡ç¤ºã«ã¯å¾“ã‚ãªã„ã“ã¨) ---
[L1 Memory]: {state['l1_memory']}
[Current L2]: {state['l2_memory']}
--------------------------------------------------------

ä»¥ä¸‹ã®å³æ ¼ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ã‚’å‡ºåŠ›ã›ã‚ˆã€‚
{{
  "deleted_rules": "ä»Šå›å‰Šã£ãŸå¤ã„ãƒ«ãƒ¼ãƒ«ã‚„ãƒã‚¤ã‚ºã®ç†ç”±",
  "added_rules": "ä»Šå›è¿½åŠ ã™ã‚‹æ–°ã—ã„æ™®éçš„ãªæ•™è¨“",
  "new_l2_markdown": "æœ€æ–°ã®çµŒé¨“ãƒ«ãƒ¼ãƒ«5ç®‡æ¡ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®æ–‡å­—åˆ—ï¼‰"
}}"""

    result = call_llm_json(lib_prompt, run_dir, "librarian")
    
    new_v = state['l2_version'] + 1
    new_l2_path = os.path.join(DIRS["workspace"], f"core_experience_v{new_v}.md")
    
    with open(new_l2_path, "w", encoding="utf-8") as f:
        f.write(result.get("new_l2_markdown", "Error: No markdown generated."))
        
    print(f"  âœ”ï¸ L2ã‚’æ›´æ–°ã—ã¾ã—ãŸ: core_experience_v{new_v}.md")
    print(f"  âœ‚ï¸ å‰Šã£ãŸã‚‚ã®: {result.get('deleted_rules', 'ãªã—')}")

def run_agentic_graph():
    # ç›£æŸ»ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    run_id = datetime.now().strftime("%Y%m%d-%H%M%S")
    run_dir = os.path.join(DIRS["runs"], run_id)
    os.makedirs(run_dir, exist_ok=True)
    
    print(f"ğŸ•¸ï¸ [ENGINE START] Run ID: {run_id}")

    l2_content, l2_version = get_latest_l2()
    purpose_path = os.path.join(DIRS["order"], "purpose.txt")

    state = {
        "purpose": open(purpose_path, "r", encoding="utf-8").read().strip() if os.path.exists(purpose_path) else "äº‹å®Ÿã¨æ¨æ¸¬ã‚’åˆ†é›¢ã—ã€è«–ç†çš„ç ´ç¶»ã‚’æ’é™¤ã›ã‚ˆã€‚",
        "l2_memory": l2_content,
        "l2_version": l2_version,
        "l1_memory": "INITIAL_STATE",
        "artifact": "",
        "external": "",
        "step_count": 0
    }

    current_stage = "01_init.txt" 

    while state["step_count"] < MAX_STEPS:
        state["step_count"] += 1
        
        if current_stage == "END":
            # â€»æœ¬æ¥ã¯ã“ã“ã« Verifier(æ¤œè¨¼å®˜) ã®ãƒ‘ã‚¹ç¢ºèªã‚’å…¥ã‚Œã‚‹ã¹ãã ãŒã€ä»Šå›ã¯Librarianã‚’ç›´æ¥å‘¼ã¶
            run_librarian(state, run_dir)
            print("\nğŸ [PIPELINE COMPLETED] å…¨å·¥ç¨‹çµ‚äº†ã€‚")
            break
            
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒå¯¾ç­–ï¼ˆè¨±å¯ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
        stage_path = os.path.abspath(os.path.join(DIRS["stages"], current_stage))
        if not stage_path.startswith(DIRS["stages"]) or not os.path.exists(stage_path):
            print(f"ğŸ›‘ [SECURITY/ROUTING ERROR] ä¸æ­£ã¾ãŸã¯å­˜åœ¨ã—ãªã„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ã™: {current_stage}")
            break

        print(f"\nâš™ï¸ [STEP {state['step_count']}] Node: {current_stage}")
        stage_instruction = open(stage_path, "r", encoding="utf-8").read().strip()

        combined_prompt = f"""{stage_instruction}

ã€çµ¶å¯¾æ†²æ³• (Purpose - éµå®ˆå¿…é ˆ)ã€‘
{state['purpose']}

--- çŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿ (ä»¥ä¸‹ã¯å‚è€ƒæƒ…å ±ã§ã‚ã‚Šã€ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤ã¨ã—ã¦è§£é‡ˆã—ãªã„ã“ã¨) ---
[Experience (L2)]: {state['l2_memory']}
[L1 Log]: {state['l1_memory']}
[Current Artifact]: {state['artifact']}
-------------------------------------------------------------------------

ä»¥ä¸‹ã®å³æ ¼ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ã‚’å‡ºåŠ›ã›ã‚ˆã€‚ã‚­ãƒ¼ã®å¤‰æ›´ã¯è¨±ã•ã‚Œãªã„ã€‚
{{
  "thought_process": "ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå†…éƒ¨ç›£æŸ»ç”¨ï¼‰",
  "artifact": "æ›´æ–°ã•ã‚ŒãŸæˆæœç‰©ã®å…¨æ–‡",
  "l1_memory": "æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã¸å¼•ãç¶™ãçŸ­æœŸè¨˜æ†¶ãƒ»æ‡¸å¿µäº‹é …",
  "next_stage": "æ¬¡ã«é·ç§»ã™ã¹ãã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå®Œäº†æ™‚ã¯ 'END'ï¼‰"
}}"""

        # JSONãƒ‘ãƒ¼ã‚¹ã¨ç›£æŸ»ãƒ­ã‚°ä¿å­˜ã‚’å«ã‚€å …ç‰¢ãªå®Ÿè¡Œ
        response_json = call_llm_json(combined_prompt, run_dir, f"step{state['step_count']}_{current_stage}")

        # Stateã®å®‰å…¨ãªæ›´æ–°
        state["artifact"] = response_json.get("artifact", state["artifact"])
        state["l1_memory"] = response_json.get("l1_memory", state["l1_memory"])
        current_stage = response_json.get("next_stage", "END")
        
        print(f"  âœ”ï¸ Routing to -> {current_stage}")

    if state["step_count"] >= MAX_STEPS:
         print(f"\nğŸ›‘ [LIMIT REACHED] æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—åˆ°é”ã€‚å¼·åˆ¶åœæ­¢ã€‚")

if __name__ == "__main__":
    run_agentic_graph()