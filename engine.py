import os
import glob
import time
from google import genai
from dotenv import load_dotenv

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(BASE_DIR, ".env"))

DIRS = {k: os.path.join(BASE_DIR, k) for k in ["order", "workspace", "stages"]}
for d in DIRS.values(): os.makedirs(d, exist_ok=True)

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
GEMINI_MODEL = os.getenv("GEMINI_MODEL")

def call_llm(prompt):
    for attempt in range(3):
        try:
            res = client.models.generate_content(model=GEMINI_MODEL, contents=prompt)
            if res.text: return res.text
            raise ValueError("Empty response")
        except Exception as e:
            if "429" in str(e): time.sleep(30)
            else: raise e
    raise RuntimeError("LLM API failed.")

def run_pipeline():
    print("ğŸš€ [ENGINE START] Universal Pipeline Processing...")
    
    # 1. ç›®çš„ï¼ˆOrderï¼‰ã®èª­ã¿è¾¼ã¿
    order_path = os.path.join(DIRS["order"], "order.txt")
    if not os.path.exists(order_path):
        print("ğŸ›‘ åœæ­¢: order.txt ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    current_context = open(order_path, "r", encoding="utf-8").read().strip()
    print(f"ğŸ“„ ORDER LOADED: {current_context[:50]}...")

    # 2. Stagesï¼ˆå¤–éƒ¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¾¤ï¼‰ã®å–å¾—ã¨é †æ¬¡å®Ÿè¡Œ
    stage_files = sorted(glob.glob(os.path.join(DIRS["stages"], "*.txt")))
    if not stage_files:
        print("ğŸ›‘ åœæ­¢: stages/ ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    for stage_file in stage_files:
        stage_name = os.path.basename(stage_file)
        print(f"\nâš™ï¸ [STAGE EXECUTING]: {stage_name}")
        
        # å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã€Œã“ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§ã®AIã®å½¹å‰²ãƒ»æ€æƒ³ãƒ»æŒ‡ç¤ºã€ã‚’èª­ã¿è¾¼ã‚€
        stage_instruction = open(stage_file, "r", encoding="utf-8").read().strip()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆæˆï¼šã€ã‚¹ãƒ†ãƒ¼ã‚¸ã®æŒ‡ç¤ºã€‘ï¼‹ã€å‰æ®µã¾ã§ã®æ–‡è„ˆ/çµæœã€‘
        combined_prompt = f"""
{stage_instruction}

ã€ç¾åœ¨ã®æ–‡è„ˆ / å‰ã‚¹ãƒ†ãƒ¼ã‚¸ã‹ã‚‰ã®å…¥åŠ›ã€‘:
{current_context}
"""
        # AIå®Ÿè¡Œ
        result = call_llm(combined_prompt)
        
        # çµæœã®ä¿å­˜ã¨ã€æ¬¡ã‚¹ãƒ†ãƒ¼ã‚¸ã¸ã®ãƒãƒˆãƒ³ã‚¿ãƒƒãƒï¼ˆContextã®æ›´æ–°ï¼‰
        out_path = os.path.join(DIRS["workspace"], f"output_{stage_name}")
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(result)
        
        current_context = result # å‡ºåŠ›ã‚’æ¬¡ã®å…¥åŠ›ã¨ã™ã‚‹ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰
        print(f"âœ”ï¸ {stage_name} å®Œäº†ã€‚çµæœã‚’workspaceã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")

    print("\nğŸ [ENGINE FINISHED] å…¨ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    run_pipeline()